{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron\n",
    "\n",
    "## Motivation: We want to classify data using a linear decision boundary\n",
    "\n",
    "## Linear Hyperplane\n",
    "\n",
    "Let $\\mathbb{R}^{M}, M \\in \\mathbb{N}$ be a M-dimensional $\\mathbb{R}$ vector space.\n",
    "\n",
    "A linear hyperplane $H(w, w_{0}) := \\{x \\in \\mathbb{R}^{M} | w^{T}x + w_{0} = 0\\}$ is a sub-space with dimension one less then the dimension of the space $\\mathbb{R}^{M}$\n",
    "\n",
    "## A linear model for a linearly-separable binary classification\n",
    "\n",
    "* Features $x \\in \\mathbb{R}^{N \\times M}$\n",
    "* Target $y_{i} \\in \\{-1, 1\\}^{N}$\n",
    "\n",
    "\n",
    "## Perceptron: Linear Classification Model\n",
    "\n",
    "### Linear classification problem:\n",
    "\n",
    "$$\n",
    "\\Large \\text{$f(.;w,w_{0}): \\mathbb{R}^{M} \\longrightarrow \\{-1, 1\\}$ with parameters $w \\in \\mathbb{R}^{M}, w_{0} \\in \\mathbb{R}$}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Large \\text{$ w^{*} = argmin_{w} \\sum_{i=1}^{N} Loss(y_{i}, f(.;w,w_{0}))$}\n",
    "$$\n",
    "\n",
    "### Linear model with a sign function:\n",
    "\n",
    "$$\n",
    "\\Large \\text{$f(x; w, w_{0}) := sign(w^{T}x + w_{0})$, with ${sign}(z) = \n",
    "\\begin{cases} \n",
    "      1 & z > 0 \\\\\n",
    "      0 & z = 0 \\\\\n",
    "      -1 & z < 0 \n",
    "   \\end{cases}$}\n",
    "$$\n",
    "\n",
    "## Geometric Interpretation of the Linear Classifier\n",
    "Classification errors can be formalized as:\n",
    "$$\n",
    "\\Large \\text{$ \\forall i: y_{i}(w^{T}x_{i}+w_{0}<0$, or $\\forall i: y_{i}sign(w^{T}x_{i}+w_{0})=-1$}\n",
    "$$\n",
    "\n",
    "## Optimizing the perceptron\n",
    "\n",
    "__Loss__ over miss-classified instances $y_{i} != sign(w^{T}x_{i}+ w_{0})$ as:\n",
    "\n",
    "$$\n",
    "\\Large \\text{$ w^{*} = argmin_{w} \\sum_{i=1}^{N} y_{i}(w^{T}x_{i} + w_{0})$}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\Large \\text{Set $ L_{i} = Loss(y_{i}, f(x_{i}; w, w_{0}))$}\n",
    "$$\n",
    "\n",
    "Then we get the Gradient:\n",
    "\n",
    "$$\n",
    "\\Large \\text{Set $ L_{i} = Loss(y_{i}, f(x_{i}; w, w_{0}))$}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
